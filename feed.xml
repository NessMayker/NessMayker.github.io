<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://nessmayker.github.io/NessMaykerChen.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://nessmayker.github.io/NessMaykerChen.github.io//" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-03T18:15:45+00:00</updated><id>https://nessmayker.github.io/NessMaykerChen.github.io//feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">What is a Neural Network?</title><link href="https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/NeuralNetBuilding/" rel="alternate" type="text/html" title="What is a Neural Network?" /><published>2023-07-03T00:00:00+00:00</published><updated>2023-07-03T00:00:00+00:00</updated><id>https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/NeuralNetBuilding</id><content type="html" xml:base="https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/NeuralNetBuilding/"><![CDATA[<p>In the most recent lesson from fast.ai’s Practical Deep Learning for Coders, we built a neural network from scratch.</p>

<h4>Neural Networks</h4>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pumpkin/neuron.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>Neural networks are computer models that are designed to mimic the human brain. The models are made up of artificial neurons that receive input, process it, and return an output. Each neuron has a weight that determines its impact and these are adjusted as the model is trained. The neural network has a hierarchal structure built from layers that process the information before passing it to the following layer. Neural networks are capable of parallel computing, where many computations are performed simultaneously, making them a very powerful modeling tool.</p>

<h4>Pumpkin Seeds Dataset</h4>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pumpkin/dataset.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The Pumpkin Seeds dataset.
</div>

<p>For this project I used the Pumpkin Seeds dataset from Kaggle. This dataset consists of different measurements of pumpkin seeds such as area, perimeter, and roundness, and whether they belong to one of two classes (‘Ürgüp Sivrisi’ and ‘Çerçevelik’). The goal is to build a model that can predict if a pumpkin seed is in the first or second class.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pumpkin/pumpkinSeeds.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The two classes of Pumpkin Seeds.
</div>

<p>I chose this dataset because the end prediction is a simple binary one; the neural network only has to sort the seed into one of two classes. I am familiar with the dataset because I used it to learn logistic regression modeling during the Erdös Institute’s data science bootcamp, and I wanted to be able to compare the performance between the different models.</p>

<h4>Baseline Model</h4>

<p>We will need something to compare all of our models to. This is called the “baseline model”. Because this problem has only two possible solutions, we can rephrase the problem to predict if the seed is in the Ürgüp Sivrisi class. In the training set 48% percent of the seeds are Ürgüp Sivrisi. Because there are less of this class than the other, we can just predict that the seed will never be in this class. Now, this seems pretty drastic, but if we always predict that the seed will never be in the class Ürgüp Sivrisi we will be right 52% of the time. Obviously this is not a good model, we just want something to compare our future models performances to.</p>

<h4>Simple Linear Model</h4>

<p>Next, we start with a simple linear model and incrementally added more complexity until we build our deep learning neural network.</p>

<p>For the linear model we try to determine the liner relationship that each feature of the dataset has with the type of seed. From the plot below we can see that the two classes of seeds have overall distinct features, although there is overlap within the sample.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pumpkin/pairplot.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Exploratory Data Analysis: different seed classes are marked in orange and blue.
</div>

<p>Here are the underlying steps of our linear model: <br />
1) Generate random coefficients <br />
2) Scale the independent features (area, perimeter, etc)<br /> 
3) Multiply the independent features by the coefficients<br />
4) Sum the result from step 3 for each seed to predict the probability it is in the class <br />
5) Calculate how off the prediction is from the true result<br />
6) Adjust the coefficients based on how they performed the last cycle<br />
7) Repeat steps 3-6 until the model’s performance stops improving.<br /></p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pumpkin/features.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The features of the dataset and how much they are positively or negatively correllated with the Ürgüp Sivrisi class.
</div>

<p>The first run of our linear model is 63% accurate. Although this is better than the baseline, it isn’t very good. We make a small change where instead of summing up the outputs to make a prediction, we run the sum of the outputs through a function called a “Sigmoid”. This function “squishes” all numbers to have a range of 0-1 and is essential in binary classification problems. When we test our linear model using the sigmoid function the accuracy jumps up to 73.6%.</p>

<p>Lets see if we can do better with a neural net!</p>

<h4>Neural Network Model</h4>

<p>Our neural network is build on top of the linear model. We start by adding a hidden layer in between out input layer and our output layer. This is similar to the left panel of the image below. Each layer performs a function on the data before passing its results to the following layer. These functions look like step 3 from the linear model, but using many more coefficients. Next the neural network updates all the layers of coefficients based on its performance. We continue training until the performance stops improving.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pumpkin/neuralNet.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Model of a neural network, image taken from https://cs231n.github.io/neural-networks-1/.
</div>

<p>Our Neural Network model scored an accuracy of 83.8%. This is a 10% improvement over our linear model!</p>

<h4>Deep Learning Neural Network</h4>

<p>The difference between a Neural Network and a Deep Learning Neural Network is that the deep learning neural network has more hidden layers. The right panel of the image above is an example of a deep learning neural network.</p>

<p>We added 10 hidden layers to our deep learning neural net model and achieved an accuracy of 86.8%, making the deep learning neural net model the most accurate out of the models we looked at so far.</p>

<h4>Comparing to Earlier Models</h4>

<p>When I worked with this dataset earlier this year, I did a lot of exploratory data analysis to find the features that seemed to be the most important to determining the seed’s class. I used a type of modeling that is similar to the linear model using the sigmoid function and was able to score an accuracy of 76-86% depending on the features I used.</p>

<p>I then revisited the dataset using several methods of model optimization which are out of the scope of this blog post, but just to say that I was able to boost the accuracy of my linear model to 88.5%, performing better than the deep neural network we just built.</p>

<p>This is just to show that there are many ways to model a problem and you don’t always have to use a computationally expensive one to get a good result.</p>]]></content><author><name></name></author><category term="DeepLearning" /><category term="DeepLearning" /><category term="NeuralNetwork" /><summary type="html"><![CDATA[Building a Deep Learning Neural Network from Scratch]]></summary></entry><entry><title type="html">Intro to Natural Language Processing (NLP)</title><link href="https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/IntroToNLP/" rel="alternate" type="text/html" title="Intro to Natural Language Processing (NLP)" /><published>2023-07-01T00:00:00+00:00</published><updated>2023-07-01T00:00:00+00:00</updated><id>https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/IntroToNLP</id><content type="html" xml:base="https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/IntroToNLP/"><![CDATA[<p>The Practical Deep Learning for Coders class introduces NLP via the U.S. Patent Phrase to Phrase Matching Kaggle Competition. This competition aims to match key phrases in patent documents in an effort to more easily identify if a patent has been described before.</p>

<p>The dataset consists of sets of two phrases that have been scored from 0-1 depending on the level of their similarity. The idea is to train a language model to predict what the phrases scores would be without knowing in advance.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/NLPpatent/Slide2.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    A glimpse into the dataset.
</div>

<p>To do this we split the dataset into a training and a testing set. The model learns from the training set and then is scored via the testing set. There are many different ways to score a model, but it is common to include a “loss” and a “metric”. The loss is a score that is meaningful to the computer, it is the difference between the predicted and actual values. The model tries to minimize that loss. The metric is used to interpret the results of the model. Some examples of metrics are accuracy or false positive rates. For this project we are using the Pearson Correlation Coefficient. This number equals 1 when the model makes a correct prediction.</p>

<p>We tackled this problem by joining the two phrases along with their patent classification entry and feeding them into a transformer. Because computers speak a different kind of language than you or I, we need to convert the text to numerical values.</p>

<p>Familiar text is often separated word for word like this Cecilia Payne-Gaposchkin quote shown on the image on the left, while unfamiliar text is often broken into smaller pieces such as the title of my last paper shown in the middle. These texts are then converted to numbers that represent indexes in the model’s vocabulary as shown on the right.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/NLPpatent/Slide1.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Examples of tokenization and numericalization of text ran through a transformer.
</div>

<p>For the first round of modeling we used the dberta transformer. This transformer is a machine learning model that has already been trained with language. This means that it should perform pretty good right out of the box! But the cool part is that we get to continue to train it more on our own dataset so that it learns to make better predictions as it becomes familiar with the data we are using.</p>

<p>When we train the model is to keep a subset of the data separate. We call this the validation set. We allow the model to assign different levels of importance (or weights) to each entry in the training set and then we score its predictions on the validation set. The model adjusts the weights based on the results from the scoring.</p>

<p>For the first model we use a validation set that is randomly chosen. For the next model we tried, we more carefully selected the validation set, keeping only entries for the validation set that did not occur in the training set. Before, any phrase might have shown up in both the training set and validation set when the validation set was randomly chosen, this time we decide to keep the phrases in the validation set separate from the training set. We ran both of these models for four training cycles.</p>

<p>The results show that as the model trains it performs slightly better over the first three epochs. The validation loss values are decreasing and the pearson value is increasing closer to 1. On the fourth epoch they seem to stop improving. Interestingly the unique values validation set did not improve the model over the long run, although for the first iteration the model performed slightly better on this validation set.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/NLPpatent/Slide3.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>I decided to play around with another transformer called patentSBERT. I was excited about this one because it is a language model trained specifically for patent documents. I repeated the same two experiments as before, one with a randomly chosen validation set and one that had only unique phrases.</p>

<p>I was surprised that the patentSBERT model did not outperform the dberta3 baseline model, especially because it was trained specifically for patent documents. Overall these results are similar to if not a little worse than the earlier rounds.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/NLPpatent/Slide4.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>To improve this model I would first try to look at different learning rates. The learning rate tells the computer how much to adjust the weights after each epoch. I would also look at different transformer models, although the baseline model seems to be working quite well.</p>

<p>One thing I have noticed in my very recent explorations into modeling, is that it isn’t always clear or intuitive how to adjust a model to improve performance. When my team participated in the CAFA5 kaggle competition last month, we were unable to ever improve on the baseline model and we weren’t exactly sure why the model was outperforming all the others that seemed to have a lot more going for them.</p>

<p>I hope that with continued study, I will learn more about model improvement and develop more intuition about the process.</p>]]></content><author><name></name></author><category term="DeepLearning" /><category term="DeepLearning" /><category term="NLP" /><summary type="html"><![CDATA[Introcuding NLP with Patent Phrase Matching]]></summary></entry><entry><title type="html">Produce Classifier</title><link href="https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/ProduceClassifier/" rel="alternate" type="text/html" title="Produce Classifier" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/ProduceClassifier</id><content type="html" xml:base="https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/ProduceClassifier/"><![CDATA[<p>I’m currently taking the <a href="https://course.fast.ai/">“Practical Deep Learning for Coders”</a> course from fast.ai and I have been so excited to practice what I have been learning.</p>

<p>For this project I am expanding on the Dog vs Cat classifier, instead training a model to classify fruit using the Fruits 360 dataset as my training data.</p>

<!-- <script type= "module"
src = "https://gradio.s3-us-west-2.amazonaws.com/3.12.0/gradio.js">
</script>

<gradio-app src="https://nessmaykerchen-produceclassifier.hf.space/"></gradio-app> -->

<iframe src="https://nessmaykerchen-produceclassifier.hf.space/" frameborder="0" width="850" height="550"></iframe>

<p>The fruits360 data set consists of 131 types of produce that are placed on a white background and have many images captured at different angles for each object. This is a decently sized training set with a total of 67,692 images, but the training set lacks diversity of different backgrounds and different groupings of multiple fruits.</p>

<p>To save time, I made a very basic model using the ResNet34 archetecture with a batch size of 16 and only one round of fine-tuning.</p>

<p>The first six example images are taken from the testing set, which the model was not allowed to interact with. As you can see the model performs brilliantly when tested with the same population of produce imaged with the same background.</p>

<p>The last two images are from a purple sweet potato I had at home. I tried to reproduce the white background for the first image, but it does have a different white balance. For the second image I used a noisy background to test how different the results would be. For the first sweet potato image, the model is split between 10 types of produce, with a preference toward labeling it as a red potato. Surprisingly, the model performed better for the second image with a split between only two types with a high certainty that it is a red potato. This is overall a good approximation considering there were no purple sweet potatoes in the training set.</p>

<p>The classifier performs much better when the images are very closely related to those in the training set. I am curious to know if the largest room for improvement in the model could be found by using a deeper neural net, or by introducing more diversity in the training set.</p>

<p>I definitely want to play around with this more in the future. For now, I need to pivot to NLP to keep up with the fast.ai course. :)</p>]]></content><author><name></name></author><category term="DeepLearning" /><category term="DeepLearning" /><category term="fastai" /><category term="ImageClassification" /><summary type="html"><![CDATA[A simple image classifier using fast.ai and the fruits360 dataset]]></summary></entry><entry><title type="html">Dog vs Cat</title><link href="https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/DogVsCat/" rel="alternate" type="text/html" title="Dog vs Cat" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/DogVsCat</id><content type="html" xml:base="https://nessmayker.github.io/NessMaykerChen.github.io//blog/2023/DogVsCat/"><![CDATA[<p>I started taking the <a href="https://course.fast.ai/">“Practical Deep Learning for Coders”</a> course from fast.ai.</p>

<p>The first two lessons introduce a simple image classifier that checks if an image is a cat or a dog. Followng the tutorial, I reproduce the classifier and make an interactive predicter using a <a href="https://huggingface.co/spaces/NessMaykerChen/CatVsDogClassifier">huggingface space</a> and <a href="https://www.gradio.app/">gradio</a>.</p>

<!-- <script type= "module"
src = "https://gradio.s3-us-west-2.amazonaws.com/3.12.0/gradio.js">
</script>

<gradio-app src="https://nessmaykerchen-catvsdogclassifier.hf.space/"></gradio-app> -->

<iframe src="https://nessmaykerchen-catvsdogclassifier.hf.space/" frameborder="0" width="850" height="550"></iframe>

<p>The classifier performs well when the images are cats or dogs, but has not been trained to classify other images, so it basically tries to measure how dog like or cat like the rest of the examples are and returns some humourous results.</p>

<p>I’m looking forward to building more sophisticated image classifiers in the future!</p>]]></content><author><name></name></author><category term="DeepLearning" /><category term="DeepLearning" /><category term="fastai" /><category term="ImageClassification" /><summary type="html"><![CDATA[A simple image classifier using fast.ai]]></summary></entry></feed>